{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udfa4 Emotion Detection through Voice (Female-only Gate)\n", "- Emotions: `happy`, `sad`, `angry`, `neutral`.\n", "- **Female-only gate**: rejects non-female voices using **pitch (F0)** check.\n",  {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["!pip install librosa soundfile sounddevice --quiet\n", "!pip install scikit-learn numpy pandas --quiet"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, math, random, pickle, io\n", "from pathlib import Path\n", "import numpy as np, pandas as pd\n", "import librosa, soundfile as sf\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import classification_report, confusion_matrix\n", "\n", "random.seed(42); np.random.seed(42)\n", "SR = 16000\n", "EMOTIONS = ['happy','sad','angry','neutral']\n", "out_models = Path('models'); out_models.mkdir(exist_ok=True)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Synthetic dataset generator\n", "We synthesize 1-second utterances that mimic emotional cues:\n", "- **happy**: higher pitch, larger energy, periodic vibrato\n", "- **sad**: lower pitch, low energy, gentle envelope\n", "- **angry**: mid/low pitch, high energy, noise burst\n", "- **neutral**: mid pitch, flat envelope\n", "\n", "This is *not* a real speech dataset, but provides a self-contained training example."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def synth_emotion(emotion: str, sr=SR, dur=1.0):\n", "    t = np.linspace(0, dur, int(sr*dur), endpoint=False)\n", "    if emotion == 'happy':\n", "        f0 = np.random.uniform(220, 280)  # higher pitch\n", "        vibrato = 0.03*np.sin(2*np.pi*6*t)\n", "        x = 0.7*np.sin(2*np.pi*f0*(t+vibrato))\n", "        env = 0.6 + 0.4*np.sin(2*np.pi*2*t)**2\n", "        x = x * env\n", "    elif emotion == 'sad':\n", "        f0 = np.random.uniform(140, 170)\n", "        x = 0.3*np.sin(2*np.pi*f0*t)\n", "        env = np.exp(-3*t)\n", "        x = x * (0.5+0.5*env)\n", "    elif emotion == 'angry':\n", "        f0 = np.random.uniform(160, 220)\n", "        tone = 0.6*np.sin(2*np.pi*f0*t)\n", "        noise = 0.4*np.random.randn(len(t))\n", "        x = tone + noise\n", "        x = np.tanh(1.5*x)\n", "    else:  # neutral\n", "        f0 = np.random.uniform(180, 220)\n", "        x = 0.5*np.sin(2*np.pi*f0*t)\n", "    # light pre-emphasis & normalize\n", "    x = np.append(x[0], x[1:] - 0.97*x[:-1])\n", "    x = x / (np.max(np.abs(x)) + 1e-6)\n", "    return x.astype(np.float32)\n", "\n", "def extract_features(y, sr=SR):\n", "    # robust framing for 1s clips\n", "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n", "    sc = librosa.feature.spectral_centroid(y=y, sr=sr)\n", "    zcr = librosa.feature.zero_crossing_rate(y)\n", "    roll = librosa.feature.spectral_rolloff(y=y, sr=sr)\n", "    # f0 estimation; fallback to NaN-safe value\n", "    try:\n", "        f0 = librosa.yin(y, fmin=80, fmax=400, sr=sr)\n", "        f0_stats = [np.nanmean(f0), np.nanstd(f0)]\n", "    except Exception:\n", "        f0_stats = [0.0, 0.0]\n", "    feat = np.hstack([\n", "        mfcc.mean(axis=1), mfcc.std(axis=1),\n", "        sc.mean(), sc.std(),\n", "        roll.mean(), roll.std(),\n", "        zcr.mean(), zcr.std(),\n", "        f0_stats\n", "    ])\n", "    return feat\n", "\n", "def build_dataset(n_per_class=400):\n", "    X, y = [], []\n", "    for emo in EMOTIONS:\n", "        for _ in range(n_per_class):\n", "            sig = synth_emotion(emo)\n", "            X.append(extract_features(sig))\n", "            y.append(EMOTIONS.index(emo))\n", "    X = np.vstack(X).astype(np.float32)\n", "    y = np.array(y)\n", "    return X, y\n", "\n", "X, y = build_dataset(n_per_class=200)\n", "X.shape, y.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train classifier"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n", "clf = Pipeline([\n", "    ('scaler', StandardScaler()),\n", "    ('rf', RandomForestClassifier(n_estimators=200, random_state=42))\n", "])\n", "clf.fit(X_train, y_train)\n", "pred = clf.predict(X_val)\n", "print(classification_report(y_val, pred, target_names=EMOTIONS))\n", "\n", "with open('models/voice_emotion_model.pkl', 'wb') as f:\n", "    pickle.dump({'pipeline': clf, 'emotions': EMOTIONS, 'sr': SR}, f)\n", "print('Saved models/voice_emotion_model.pkl')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Female-only gate helper\n", "At inference, we estimate **median F0** using `librosa.yin`. If **median F0 is not in [165, 255] Hz**, we reject as **non-female** (prompt user to upload a female voice)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def is_female_voice(y, sr=SR, fmin=80, fmax=400):\n", "    try:\n", "        f0 = librosa.yin(y, fmin=fmin, fmax=fmax, sr=sr)\n", "        f0_med = np.nanmedian(f0)\n", "        return 165 <= f0_med <= 255, f0_med\n", "    except Exception:\n", "        return False, np.nan\n", "\n", "# quick demo on synthetic happy (likely high pitch)\n", "demo = synth_emotion('happy')\n", "print('Female gate check:', is_female_voice(demo)[0])"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}
