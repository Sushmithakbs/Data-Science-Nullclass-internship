{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83c\udfac Age & Emotion Detection for Horror Movie Theatre (Rule-Based Gate)\n", "\n", "**Goal**: Real-time system that detects **age** for each face. If **age < 13** or **age > 60** \u2192 show **\"Not allowed\"** and draw **red** box. If **13 \u2264 age \u2264 60** \u2192 also predict **emotion**.\n", "\n", "All predictions (age, emotion if applicable, entry time) are **logged to CSV**.\n", "\n", "\ud83d\udc49 This notebook **creates its own ML model**: a compact CNN trained quickly on a **synthetic face-like dataset** (so it runs anywhere without downloading large datasets). You can later swap in a real dataset (e.g., UTKFace for age + FER2013 for emotion) using the data loaders provided.\n", "\n", "### What you get\n", "- **Custom PyTorch model** with a shared backbone and two heads: **age regression** + **emotion classification** (4 classes: `neutral, happy, sad, scared`).\n", "- Fast training on a small **synthetic dataset** (generated on the fly).\n", "- **OpenCV**-based face detection (Haar cascade) for real-time/video/image inference.\n", "- **CSV logging**: `logs/theatre_entries.csv` with columns `[timestamp, person_id, age_pred, emotion_pred, allowed]`.\n", "\n", "---\n", "## Quick Start\n", "1. Run the **Install** cell.\n", "2. Run **Create Synthetic Dataset** (or plug in your own data path).\n", "3. Run **Train** (few epochs).\n", "4. Run **Live/Video/Image Inference**.\n", "\n", "**Note**: For headless environments (e.g., Colab), set `SHOW_WINDOWS=False` to avoid GUI errors; frames will be saved to `out/`.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["!pip install torch torchvision torchaudio --quiet\n", "!pip install opencv-python pandas scikit-learn --quiet"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import os, math, time, csv, random, shutil\n", "from datetime import datetime\n", "import numpy as np\n", "import pandas as pd\n", "import cv2\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import Dataset, DataLoader\n", "from torchvision import transforms\n", "from sklearn.model_selection import train_test_split\n", "\n", "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "SEED = 42\n", "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n", "\n", "os.makedirs('data/synth/train', exist_ok=True)\n", "os.makedirs('data/synth/val', exist_ok=True)\n", "os.makedirs('models', exist_ok=True)\n", "os.makedirs('logs', exist_ok=True)\n", "os.makedirs('out', exist_ok=True)\n", "\n", "SHOW_WINDOWS = False  # set True if running locally with display\n", "HAAR_FACE = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n", "FACE_CASCADE = cv2.CascadeClassifier(HAAR_FACE)\n", "EMOTION_LABELS = ['neutral','happy','sad','scared']\n", "IMG_SIZE = 64\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Synthetic dataset generator (runs anywhere)\n", "We procedurally draw simple face-like emojis with parameters that correlate with **age** (wrinkles/skin texture) and **emotion** (mouth/eyebrow shapes)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def draw_face(age: int, emotion: str, size=IMG_SIZE):\n", "    img = np.zeros((size, size, 3), dtype=np.uint8) + 255\n", "    # base face ellipse\n", "    cv2.ellipse(img, (size//2, size//2), (size//3, size//3), 0, 0, 360, (220, 200, 180), -1)\n", "    # eyes\n", "    eye_y = size//2 - size//6\n", "    eye_x_off = size//6\n", "    cv2.circle(img, (size//2 - eye_x_off, eye_y), size//20, (0,0,0), -1)\n", "    cv2.circle(img, (size//2 + eye_x_off, eye_y), size//20, (0,0,0), -1)\n", "    # mouth based on emotion\n", "    mx, my = size//2, size//2 + size//8\n", "    w = size//4\n", "    if emotion == 'happy':\n", "        cv2.ellipse(img, (mx,my), (w, w//3), 0, 0, 180, (0,0,0), 2)\n", "    elif emotion == 'sad':\n", "        cv2.ellipse(img, (mx,my+4), (w, w//3), 0, 180, 360, (0,0,0), 2)\n", "    elif emotion == 'scared':\n", "        cv2.circle(img, (mx,my), w//4, (0,0,0), 2)\n", "    else:\n", "        cv2.line(img, (mx-w, my), (mx+w, my), (0,0,0), 2)\n", "    # age cues: wrinkles for older, smooth for young\n", "    if age > 50:\n", "        for k in range(3):\n", "            y = size//2 + k*4\n", "            cv2.line(img, (mx-w//2, y), (mx+w//2, y), (160,140,120), 1)\n", "    if age < 16:\n", "        cv2.circle(img, (size//2 - eye_x_off, eye_y+10), 3, (255,180,180), -1)\n", "        cv2.circle(img, (size//2 + eye_x_off, eye_y+10), 3, (255,180,180), -1)\n", "    # slight noise to diversify\n", "    noise = np.random.normal(0, 3, img.shape).astype(np.int16)\n", "    img = np.clip(img.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n", "    return img\n", "\n", "class SynthFaceDataset(Dataset):\n", "    def __init__(self, n=2000, transform=None):\n", "        self.transform = transform\n", "        self.ages = np.random.randint(3, 85, size=n)\n", "        self.emotions = np.random.choice(EMOTION_LABELS, size=n)\n", "    def __len__(self):\n", "        return len(self.ages)\n", "    def __getitem__(self, idx):\n", "        age = int(self.ages[idx])\n", "        emo = str(self.emotions[idx])\n", "        img = draw_face(age, emo)\n", "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n", "        x = torch.tensor(img).permute(2,0,1).float()/255.0\n", "        age_norm = torch.tensor([age/100.0], dtype=torch.float32)\n", "        emo_idx = torch.tensor(EMOTION_LABELS.index(emo), dtype=torch.long)\n", "        return x, age_norm, emo_idx\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model: Tiny CNN with shared backbone + 2 heads"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import torch.nn as nn\n", "\n", "class TinyBackbone(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.conv = nn.Sequential(\n", "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n", "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n", "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n", "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1,1))\n", "        )\n", "    def forward(self, x):\n", "        x = self.conv(x)\n", "        x = x.view(x.size(0), -1)\n", "        return x\n", "\n", "class AgeEmotionNet(nn.Module):\n", "    def __init__(self, num_emotions=4):\n", "        super().__init__()\n", "        self.backbone = TinyBackbone()\n", "        self.age_head = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64,1))\n", "        self.emo_head = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64,num_emotions))\n", "    def forward(self, x):\n", "        feat = self.backbone(x)\n", "        age = self.age_head(feat)\n", "        emo = self.emo_head(feat)\n", "        return age, emo\n", "\n", "model = AgeEmotionNet(4)\n", "sum(p.numel() for p in model.parameters())\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "BATCH = 64; LR = 1e-3; EPOCHS = 5\n", "train_ds = SynthFaceDataset(2000)\n", "val_ds = SynthFaceDataset(400)\n", "train_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True)\n", "val_dl = DataLoader(val_ds, batch_size=BATCH)\n", "model = AgeEmotionNet(4).to(DEVICE)\n", "opt = torch.optim.Adam(model.parameters(), lr=LR)\n", "mse, ce = nn.MSELoss(), nn.CrossEntropyLoss()\n", "\n", "def one_epoch(dl, train=True):\n", "    model.train(train)\n", "    tot, n = 0.0, 0\n", "    for x, age, emo in dl:\n", "        x, age, emo = x.to(DEVICE), age.to(DEVICE), emo.to(DEVICE)\n", "        if train: opt.zero_grad()\n", "        age_p, emo_p = model(x)\n", "        loss = mse(age_p, age) + ce(emo_p, emo)\n", "        if train: loss.backward(); opt.step()\n", "        tot += loss.item()*x.size(0); n += x.size(0)\n", "    return tot/max(n,1)\n", "\n", "for e in range(1, EPOCHS+1):\n", "    tr = one_epoch(train_dl, True)\n", "    va = one_epoch(val_dl, False)\n", "    print(f\"Epoch {e}/{EPOCHS}  train:{tr:.4f}  val:{va:.4f}\")\n", "torch.save(model.state_dict(), 'models/age_emo_synth.pt')\n", "print('Saved to models/age_emo_synth.pt')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Inference & CSV logging"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["CSV_LOG = 'logs/theatre_entries.csv'\n", "FACE_CASCADE = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n", "\n", "def load_model(path='models/age_emo_synth.pt'):\n", "    m = AgeEmotionNet(4).to(DEVICE)\n", "    m.load_state_dict(torch.load(path, map_location=DEVICE))\n", "    m.eval(); return m\n", "\n", "def predict_one_face(face_bgr, m):\n", "    face_rgb = cv2.cvtColor(cv2.resize(face_bgr, (64,64)), cv2.COLOR_BGR2RGB)\n", "    x = torch.tensor(face_rgb).permute(2,0,1).float()/255.0\n", "    with torch.no_grad():\n", "        age_p, emo_p = m(x.unsqueeze(0).to(DEVICE))\n", "        age = float(np.clip(age_p.item()*100.0, 0, 100))\n", "        emo = ['neutral','happy','sad','scared'][int(torch.argmax(emo_p,1))]\n", "    return age, emo\n", "\n", "def log_entry(person_id, age, emotion_or_na, allowed):\n", "    t = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n", "    exists = os.path.exists(CSV_LOG)\n", "    with open(CSV_LOG, 'a', newline='') as f:\n", "        w = csv.writer(f)\n", "        if not exists:\n", "            w.writerow(['timestamp','person_id','age_pred','emotion_pred','allowed'])\n", "        w.writerow([t, person_id, f\"{age:.1f}\", emotion_or_na, int(allowed)])\n", "\n", "def annotate_frame(frame, m, next_id_start=1):\n", "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n", "    faces = FACE_CASCADE.detectMultiScale(gray, 1.2, 5)\n", "    pid = next_id_start\n", "    for (x,y,w,h) in faces:\n", "        face = frame[y:y+h, x:x+w]\n", "        age, emo = predict_one_face(face, m)\n", "        allowed = (13 <= age <= 60)\n", "        color = (0,255,0) if allowed else (0,0,255)\n", "        label = f\"{age:.0f} - {emo if allowed else 'Not allowed'}\"\n", "        cv2.rectangle(frame, (x,y), (x+w, y+h), color, 2)\n", "        cv2.putText(frame, label, (x, max(0,y-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n", "        log_entry(pid, age, emo if allowed else 'N/A', allowed)\n", "        pid += 1\n", "    return frame\n", "\n", "model = load_model()\n", "\n", "# Example usage:\n", "# img = cv2.imread('some_image.jpg'); out = annotate_frame(img, model); cv2.imwrite('out/result.jpg', out)\n", "# For video/webcam, use the helper below:\n", "def run_source(src=0, show_windows=False):\n", "    cap = cv2.VideoCapture(src)\n", "    i=0\n", "    while True:\n", "        ret, frame = cap.read();\n", "        if not ret: break\n", "        out = annotate_frame(frame, model, 1)\n", "        if show_windows:\n", "            cv2.imshow('Theatre Gate', out)\n", "            if cv2.waitKey(1) & 0xFF == 27: break\n", "        else:\n", "            if i % 30 == 0:\n", "                cv2.imwrite(f'out/frame_{i:05d}.jpg', out)\n", "        i += 1\n", "    cap.release();\n", "    if show_windows: cv2.destroyAllWindows()\n", "\n", "# run_source(0, show_windows=False)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}